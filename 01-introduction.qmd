# Introduction to Prediction Modeling in HRM 

Statistical modeling in the social sciences serves two distinct yet complementary purposes: explanation and prediction. Explanatory modeling, the dominant paradigm in Human Resource Management (HRM), focuses on testing theoretical propositions by estimating associations among constructs and evaluating in-sample model fit through metrics such as R², F-tests, and other standard indices [@Shmueli2010]. In contrast, predictive modeling assesses a model’s capacity to generate accurate forecasts for new, unseen observations, typically using out-of-sample performance indicators such as the root mean squared error (RMSE)[@ShmueliKoppius2011]. 

This distinction carries important methodological consequences. While explanatory analysis emphasizes statistical significance and causal interpretation, predictive modeling prioritizes generalizability and accuracy, often employing techniques such as train–test splits or k-fold cross-validation to evaluate performance on new data [@Hastie2013]. Scholars have long warned against conflating explanation with prediction, as models optimized for in-sample performance may overfit the data and yield misleading insights when applied to new contexts [@Forster2002; @ForsterSober1994]. 

This issue is particularly relevant in applied fields such as HRM, where empirical research frequently informs managerial decision-making. Prescriptive statements, such as the assumption that implementing a specific HR practice will enhance employee outcomes, implicitly rely on predictive validity. Yet, as Sarstedt and Danks (2021) highlight, a significant gap exists between these predictive ambitions and the methodological tools used in the field. In their review of the HRM literature, they find that while nearly all studies put forward practical recommendations, they rely exclusively on explanatory metrics for validation. This disconnect raises critical concerns about the real-world utility and robustness of the findings [@Sarstedt2021]. 

## Problem statement  

The present thesis addresses this prediction–explanation gap by empirically assessing the extent to which explanatory performance translates into predictive accuracy. Specifically, we replicate and extend the job satisfaction model originally proposed by Drabe et al. (2015) using data from the International Social Survey Programme (ISSP) 2015. Our analysis focuses on three national contexts, namely Germany, Japan, and the United States, which represent leading industrial economies in Europe, Asia, and North America respectively, to examine both within-sample fit and cross-contextual generalizability. 
 
We compare explanatory and predictive model performance using in-sample R² and both in-sample and out-of-sample RMSE. In doing so, we investigate (1) whether   a model that fits its training data well also performs well in predicting new data from contextually similar samples, and (2) whether a model with strong explanatory power can generalize effectively across different national contexts. 

In addition to this comparative evaluation, we integrate Necessary Condition Analysis (NCA) to identify structural prerequisites for achieving high job satisfaction. Unlike conventional regression approaches, which estimate average effects, NCA allows us to explore whether certain conditions are essential for the outcome to occur at all. By combining these perspectives, we aim to present a more comprehensive understanding of the methodological requirements for building robust and generalizable models in HRM research. 
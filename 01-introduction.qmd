# Introduction to Prediction Modeling in HRM 

Statistical modeling in the social sciences serves two distinct but complementary purposes: explanation and prediction. Explanatory modeling, the dominant paradigm in Human Resource Management (HRM), focuses on testing theoretical propositions by estimating associations among constructs and evaluating in-sample fit via statistics such as R², F-tests, and SEM indices (e.g., CFI, RMSEA)[^1] [@Shmueli2010]. In contrast, predictive modeling assesses a model’s capacity to generate accurate forecasts for new, unseen observations using out-of-sample performance metrics like root mean squared error (RMSE) and mean absolute error (MAE) [@ShmueliKoppius2011]. The distinction has profound methodological implications; explanatory analyses prioritize causal inference and parameter significance, whereas predictive analyses emphasize generalizability and error minimization, often employing train–test splits or cross-validation techniques [@Hastie2013]. Scholars have long cautioned that conflating explanation with prediction may lead to overfitting [^2] and misleading inferences when models optimized for in-sample performance fail to generalize beyond the estimation dataset [@Forster2002; @ForsterSober1994]. 

This distinction is especially critical in an applied field like HRM, where research often culminates in managerial recommendations. Such prescriptions—implying that “If an organization implements practice X, then outcome Y will improve”—are inherently predictive. They presuppose that the underlying statistical model can reliably forecast how Y will change when X is altered. Yet, a significant gap exists between this predictive goal and the field's methodological practices. In a landmark review, Sarstedt and Danks (2021) demonstrated that while 99% of HRM studies advance prescriptive claims, they rely solely on explanatory metrics for model validation. This creates a fundamental "gap between rhetoric and reality," calling into question the real-world utility of many research-based recommendations [@Sarstedt2021]. 

The present thesis aims to replicate and extend Sarstedt and Danks’s (2021) investigation, empirically demonstrating the gap between explanatory and predictive modeling in HRM. In particular, we are showing that models with similar degrees of explanatory power can perform very differently in term of predictive power. In addition, we are going to demonstrate that a model developed in one context, for example in a country, can not necessarly generalize well if applied to other contexts. Using the International Social Survey Programme (ISSP) 2015 Work Orientation dataset, we re-estimate a job satisfaction model originally proposed by Drabe et al. (2015) across three distinct national contexts: the United States, Germany, and Japan. Through a systematic evaluation of in-sample R² and out-of-sample RMSE and MAE—employing train–hold-out splits and k-fold cross-validation—we seek to illustrate the variability of predictive performance relative to explanatory fit. Furthermore, we integrate Necessary Condition Analysis (NCA) to identify conditions that must be met for achieving a certain level of job satisfaction, thereby enriching explanatory insights with boundary constraints on outcome attainment. 

## Framing the Prediction–Explanation Problem in HRM Research 

Despite the increasing use of statistical models in HRM research, a critical discrepancy persists between the field’s explanatory practices and its implicit predictive claims. While theoretical frameworks are commonly tested using in-sample model fit metrics, managerial recommendations derived from such models often imply predictive validity, i.e., the capacity to generalize effects beyond the estimation sample. As Sarstedt and Danks (2021) emphasize, this creates a methodological gap that compromises the practical utility of many empirical findings. 

To address this issue, the present seminar thesis replicates the job satisfaction model developed by Drabe et al. (2015), which analyses the relationship between job-related values and job satisfaction across aging workforces in the United States, Germany, and Japan. Building on Sarstedt and Danks’s (2021) critique, we aim to compare explanatory and predictive model performance using both in-sample (R²) and out-of-sample (RMSE) metrics. In doing so, we investigate whether a model with strong explanatory power can generalize effectively across different national contexts. 

Our methodological approach includes re-estimating the original regression models using R and Python, conducting predictive evaluations via train–test splits and k-fold cross-validation, and incorporating Necessary Condition Analysis (NCA) to identify essential predictors for achieving high job satisfaction. Additionally, we perform assumption diagnostics for the linear regression models in JASP to ensure the validity of our estimation procedures. Through this triangulated approach, we seek to demonstrate the limits of conventional explanatory models and the added value of predictive and necessity-based perspectives in HRM research. 

[^1]: *Structural Equation Modeling (SEM) fit indices, such as the Comparative Fit Index (CFI) and Root Mean Square Error of Approximation (RMSEA), evaluate how well a proposed model reproduces observed data patterns. Acceptable model fit is typically indicated by CFI values ≥ .90 and RMSEA values ≤ .08* 

[^2]: *Overfitting occurs when a model captures noise unique to the estimation dataset rather than the true underlying patterns. Predictive validation techniques, such as train/test splits or k-fold cross-validation (CV), help detect overfitting and mitigate the risk of generating misleading managerial insights.* 
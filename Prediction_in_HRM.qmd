---
title: "Prediction in Human Recourse Managment"
subtitle: "A gab between rhetoric and reality"

author: 
    - name: "Abhinna Shah"
    - name: "Ariel Jeremi Wowor"
    - name: "Bianca Ricci"
    - name: "Marcel Dieti"

abstract-title: "Abstract"
abstract: "we will write a abstract here at the very end."

date: today

format: pdf
toc: true

editor: visual
bibliography: references.bib
---

# Introduction

Human Resource Management (HRM) research frequently employs quantitative models to explain relationships among organizational practices and employee outcomes. Explanatory modeling—grounded in inferential statistics and theory testing—has dominated the field, with scholars emphasizing in‐sample fit indices such as R², F‐statistics, and structural equation‐model (SEM) fit measures (e.g., CFI, RMSEA) to validate theoretical propositions. By contrast, predictive modeling focuses on out‐of‐sample performance, assessing a model’s ability to forecast new observations via metrics such as root mean squared error (RMSE) and mean absolute error (MAE) [@Sarstedt2021]. While both approaches serve distinct scientific objectives—explanation versus forecasting—their integration is crucial for generating robust, practically relevant insights in HRM.

HRM studies often culminate in managerial recommendations—statements implying “If organizations implement practice X, then outcome Y will improve.” Such prescriptions presuppose that the estimated model reliably forecasts Y when X changes. Without explicit predictive validation, overfitting [^1] to the original sample can lead scholars and practitioners to overestimate an intervention’s real‐world impact. Sarstedt and Danks (2021) showed that 99% of HRM studies report only explanatory metrics, yet advance prescriptive claims—creating a methodological gap between rhetoric and reality [@Sarstedt2021].

[^1]: Overfitting occurs when a model captures random noise specific to the estimation dataset rather than the underlying phenomenon. Predictive validation (e.g., train/test split, k‐fold CV) quantifies and often reveals such overfitting, safeguarding against spurious managerial guidance.

# Theoretical Foundations

## Explanation versus Prediction in HRM Research

Modeling serves two distinct but complementary purposes: explanation and prediction. Explanatory modeling focuses on testing theoretical propositions by estimating associations among constructs and evaluating in‐sample fit via statistics such as R², F‐tests, and SEM indices (e.g., CFI, RMSEA) [@Shmueli2010]. In contrast, predictive modeling assesses a model’s capacity to generate accurate forecasts for new, unseen observations using metrics such as root mean squared error (RMSE) and mean absolute error (MAE)[@ShmueliKoppius2011]. The distinction has profound methodological implications: explanatory analyses prioritize causal inference and parameter significance, whereas predictive analyses emphasize out‐of‐sample generalizability and error minimization, often employing train–test splits or cross‐validation techniques (e.g., k‐fold, LOOCV) [@Hastie2013]. Scholars have long cautioned that conflating explanation with prediction may lead to overfitting and misleading inferences when models optimized for in‐sample performance fail to generalize beyond the estimation dataset[@Forster2002][@ForsterSober1994].

# Research Objectives and Contribution

## Aims of the Replication Study

The present thesis aims to replicate and extend Sarstedt and Danks’s (2021) investigation, empirically demonstrating the gap between explanatory and predictive modeling in HRM. Using the International Social Survey Programme (ISSP) 2015 Work Orientation dataset, we re‐estimate a job satisfaction model originally proposed by Drabe et al. (2015) across three distinct national contexts: the United States, Germany, and Japan. Through a systematic evaluation of in‐sample R² and out‐of‐sample RMSE and MAE—employing train–hold‐out splits and k‐fold cross‐validation—we seek to illustrate the variability of predictive performance relative to explanatory fit. Furthermore, we integrate Necessary Condition Analysis (NCA) to identify conditions that must be present for high job satisfaction, thereby enriching explanatory insights with boundary constraints on outcome attainment.

# Data Source and Variable Description

## ISSP 2015 Work Orientation Dataset

The ISSP 2015 Work Orientation module provides harmonized, publicly available survey data from multiple countries. We selected three national subsamples—United States (USA; n = 915), Germany (GER; n = 899), and Japan (JPN; n = 635)—to ensure cultural and institutional diversity. The dependent variable, job satisfaction (job_sat), was measured on a 1–7 Likert scale. Independent variables matched those used by Drabe et al. (2015): gender (sex), age category (Age_cat: ≤35, 36–49, ≥50), years of education (educyrs), income, perceived advancement opportunities (advancement), job security (security), job interest (interesting), work autonomy (independent), and quality of relationships with management (rel_mgmt) and colleagues (rel_clgs) [@Drabe2015].

\
Key Constructs and Measurement

The model replicates Drabe et al. (2015) and includes:

1.  Gender (sex) – intrinsic demographic

2.  Age category (Age_cat: ≤35, 36–49, ≥50) – intrinsic demographic

3.  Education (educyrs) – intrinsic human‐capital

4.  Income – extrinsic reward

5.  Advancement opportunities (advancement) – extrinsic reward

6.  Job security (security) – extrinsic reward

7.  Job interest (interesting) – intrinsic job characteristic

8.  Autonomy (independent) – intrinsic job characteristic

9.  Relationship with management (rel_mgmt) – social work context

10. Relationship with colleagues (rel_clgs) – social work context

11. Job satisfaction (job_sat) – outcome variable

# Data Preprocessing and Preparation

## Handling Missing Data and Case‐Wise Deletion

Survey items exhibited uneven missingness across countries (up to 35% for some items). In line with Sarstedt and Danks (2021), we applied case‐wise deletion to maintain consistent sample composition for comparative modeling, accepting potential reductions in statistical power to preserve internal validity. All continuous predictors were standardized (z‐scores) to facilitate coefficient comparability across models.

## Reverse Coding and Scale Alignment

In the ISSP survey, higher numerical responses sometimes indicate less favorable conditions (e.g., “1 = Very satisfied” to “7 = Very dissatisfied”). To ensure that all predictors align directionally with job satisfaction (higher = better), we reversed scales so that greater values consistently denote more positive levels. This simplifies interpretation: a positive regression coefficient uniformly implies that increases in the predictor raise satisfaction.

# Methodological Framework

## Explanatory Modeling via Multiple Regression

## Predictive Modeling and Cross‐Validation

A single train/test split divides data once into estimation and validation samples, providing one out‐of‐sample error estimate. However, this estimate can be sensitive to how the split occurs. K‐fold cross‐validation (e.g., k = 10) partitions the data into k subsets, iteratively training on k−1 folds and validating on the remaining fold. Averaging errors across all folds yields a more stable and less split‐dependent estimate of predictive performance [@Hastie2013].

## Evaluation Metrics: R², RMSE, MAE {#evaluation-metrics}

**R²** measures the proportion of variance in the dependent variable explained by the model **within the estimation sample**. It ranges from 0 to 1, with higher values indicating better in‐sample fit.

**RMSE** is the square root of the average squared prediction errors and retains the dependent variable’s scale. It conveys, on average, how far predictions deviate from actual values.

**MAE** is the average absolute prediction error, also in the outcome’s scale, offering an easily interpretable mean deviation.

While R² speaks to explanatory adequacy, RMSE and MAE quantify predictive accuracy on new data. A model can exhibit a high R² but still produce large RMSE, signaling overfitting.

## Necessary Condition Analysis (NCA)

NCA identifies variables that constitute **prerequisites** for achieving a certain outcome level. A necessary condition is one without which the outcome cannot occur, even if it alone is insufficient to guarantee the outcome[@Richter2025]. NCA detects “empty zones” in scatterplots—regions above a ceiling line where no observations exist—indicating that below a threshold of the predictor, the outcome never reaches the target.

If years of education (X) is a necessary condition for job satisfaction ≥ 6 (Y), NCA might show that no respondents with fewer than 8 years of education report Y ≥ 6. Despite other favorable conditions, education below this threshold precludes high satisfaction.

# Diagnostics and Assumption Checks

## Linearity, Homoscedasticity, Normality

## Collinearity Diagnostics and VIF

# Replication Results

In this chapter, we build on the models developed in the previous sections using the ISSP dataset to explore two key ideas.\
First, we examine whether a model that fits its training data well (i.e., with high explanatory power) also performs well in predicting new data. Second, we assess whether a model trained in one specific context (such as one country) can generalize to different contexts.\
All analyses were conducted in R and later replicated in Python to ensure robustness. Since results were consistent across both environments, we report only the R-based plots here for clarity.

## Model Variability within Context

To address the first question, we focused on the German dataset and built 1000 models using repeated random sampling. In each replication, we drew a sample of 500 observations, then split it into two equally sized subsets: one for training and one as a holdout set.\
Each model was trained on its training set, and we computed two in-sample metrics to evaluate explanatory power: R-squared and root mean square error (RMSE). We then used the same model to generate predictions on the holdout set and calculated the out-of-sample RMSE as a measure of predictive power.\
Figures 1 and 2 illustrate how these two types of performance relate. In Figure 1, we plot R-squared (x-axis) against out-of-sample RMSE (y-axis); in Figure 2, the x-axis is the in-sample RMSE.\
In both plots, we observe that models with similar in-sample performance can vary widely in their out-of-sample predictive accuracy. This means that high explanatory power does not guarantee high predictive power.\
Interestingly, Figure 1 shows that higher R-squared values tend to be associated with higher prediction error, suggesting possible overfitting. Similarly, Figure 2 shows a negative trend between in-sample and out-of-sample RMSE: models that fit the training data very well often perform worse on new data.

To explore this further, we zoomed in on the models with an R-squared close to that of the original German model (R² between 0.366 and 0.386). Although these models all explain roughly the same proportion of variance, their prediction errors still vary greatly, from 0.75 to 0.94, meaning a 25% increase in error between the best and worst cases.

![Relationship between (in‐sample) R² and out‐of‐sample root mean square error (RMSE) ](images/Fig1a.jpg)

![Relationship between in‐sample RMSE and out‐of‐sample RMSE](images/Fig1b-2.jpg)

![Density plot of predictive power for subsamples with 0.366 \< R² \< 0.386. RMSE, root mean square error](images/Fig 3.jpg)

This first analysis highlights the importance of jointly evaluating both explanatory and predictive power when assessing a model’s performance. Our goal is to caution researchers against drawing prescriptive conclusions based solely on a model’s in-sample fit, such as R-squared or in-sample RMSE, without verifying how well the model performs on unseen data.\
As we have shown, models that explain a large proportion of variance in the training data do not necessarily perform well in predicting new observations. In fact, we observed that some models with high explanatory power exhibited relatively poor predictive performance, indicating a risk of overfitting.\
A well-designed model should not only provide a good fit to its own data but also be able to generalize to new samples. However, there is no fixed relationship between in-sample metrics (e.g., R-squared, RMSE) and out-of-sample performance, which makes the predictive evaluation an essential step.\
While explanatory modelling focuses on understanding relationships between variables, through the significance, direction, and size of coefficients, predictive modelling aims to accurately forecast new outcomes. These two goals are distinct, but not mutually exclusive.\
By adopting an EP (Explanatory and Predictive) approach, researchers can strike a balance: developing models that both explain the data and support reliable prescriptive insights. Ultimately, when the objective is to derive actionable conclusions, predictive power must be explicitly assessed, explanatory power alone is not enough.

## Cross Country Generalizability of Predictive Power

In the second part of our analysis, we explore whether a model trained in one country can predict outcomes in other countries.\
We used the full ISSP data for Germany, the USA, and Japan, building one model per country. The bar chart in Figure 4 shows the R-squared of each model on its own country’s data, indicating how well it explains variation within its own context.\
The heatmap in Figure 5 shows the out-of-sample RMSE when each country-specific model is used to predict data from the other countries. This allows us to compare generalizability across contexts.\
From the results, the Japanese model shows to have the highest R-squared (0.508), meaning it fits its own data well. However, it performs poorly when predicting the USA data, suggesting it may be overfitting to context-specific patterns that don’t transfer.\
Furthermore, no other model predicts the Japanese data well, indicating that job satisfaction in Japan may follow idiosyncratic patterns not captured by models trained elsewhere.\
The German model has the lowest explanatory power (R² = 0.384), but its data are reasonably well predicted by both the USA and Japan models. This may mean that the German model omits some relevant predictors or suffers from multicollinearity. At the same time, the predictors that matter in the USA and Japan might partially capture variation in the German context.

![Ability of each country-specific model to fit its own country data](images/Figure 4.jpg)

![Predictive power of country-specific models](images/Figure 5.jpg)

This second analysis shows that models trained in one country do not necessarily generalize well to other countries. Patterns that hold in one cultural or institutional context may not apply elsewhere.\
We emphasize the importance of validating predictive performance across time, populations, or geographies, especially when a model is intended for broader use. Testing models on external data is key to understanding their generalizability and reliability.

# Extension: Determinants of Job Satisfaction

## Standardized Coefficients across Countries

## Identifying Necessary Conditions with NCA

# Discussion

## Implications for HRM Theory and Practice

## Challenges in Reproducing Published Analyses

## Comparison of Python, R/JASP, and SmartPLS Outcomes

# Conclusion and Future Research

## Summary of Key Findings

## Limitations of the Current Study

## Suggestions for Advancing Predictive Rigour in HRM